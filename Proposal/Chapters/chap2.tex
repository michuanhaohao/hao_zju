\chapter{研究现状与文献综述}

本章节主要介绍本课题相关的研究现状，包括行人重识别(Person re-identification, person ReID)和跨摄像头多目标跟踪(Multi-target multi-camera tracking, MTMC tracking)两个部分。
在本章节将会分别介绍这两个子课题相关的数据集和现有算法。
其中行人重识别着重介绍近几年深度卷积神经网络相关的方法，而跨摄像头多目标跟踪将会着重介绍基于行人重识别的方法。

\section{行人重识别}

行人重识别也称行人再识别，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术。
广泛被认为是一个图像检索的子问题。
给定一个监控行人图像，检索跨设备下的该行人图像。
旨在弥补目前固定的摄像头的视觉局限，并可与行人检测/行人跟踪技术相结合，可广泛应用于智能视频监控、智能安保等领域。

而对于跨摄像头多目标跟踪问题，当一个行人目标在其中一个摄像头视野中消失后，要把该行人在其他摄像头中再次识别出来，这就是典型的行人重识别问题。
也就是说，行人重识别技术是跨摄像头多目标跟踪的基础。
因此，在本小节将会先介绍现有的行人重识别相关的数据集、准确度评估准则和一些现有的主流方法。

\subsection{相关数据集}
行人重识别相关的数据集总共有十几个，在早年深度学习还未出现的时候，那时的数据集图片数量还比较少。
随着深度学习的诞生，行人重识别问题对数据量的要求大大增加，本小节将介绍几个适用深度学习的大规模行人出识别数据集。

\begin{itemize}
\item{Market1501}

Market1501 \cite{zheng2015scalable}是在清华大学校园中采集，图像来自6个不同的摄像头，其中有一个摄像头为低像素。
同时该数据集提供训练集和测试集。
训练集包含12,936张图像，测试集包含19,732 张图像。
图像由检测器自动检测并切割，包含一些检测误差（接近实际使用情况）。
训练数据中一共有751人，测试集中有750 人。
所以在训练集中，平均每类（每个人）有17.2张训练数据。

\item{MARS}

MARS (Motion Analysis and Re-identification Set) \cite{zheng2016mars}数据集是Market1501的扩展。
该数据集的图像由检测器自动切割，包含了行人图像的整个跟踪序列(tracklet)。
MARS总共提供1,267个行人的20,478个图像序列，和Market1501一样来自同样的6个摄像头。
和其他单帧图像数据集不一样的地方是，MARS是提供序列信息的大规模行人重识别数据集。

\item{CUHK03}

CUHK03 \cite{Li2014DeepReID} 在香港中文大学采集，图像来自2个不同的摄像头。
该数据集提供机器自动检测和手动检测两个数据集。
其中检测数据集包含一些检测误差，更接近实际情况。
数据集总共包括1,467个行人的14,097张图片，平均每个人有9.6张训练数据。

\item{CUHK-SYSU}

CUHK-SYSU\cite{xiao2016end}是香港中文大学和中山大学一起收集的数据集。
该数据集的特点是提供整个完整的图片，而不像其他大部分数据集一样只提供自动或者手动提取边框(bounding box)的行人图片。
该数据集总共包括18,184张完整图片，内含8,432个行人的99,809张行人图片。
其中训练集有11,206张完整图片，包含5,532个行人。
测试集有6,978张完整图片，包含2,900个行人。

\item{DukeMTMC-reID}

DukeMTMC-reID \cite{ristani2016MTMC}在杜克大学内采集，图像来自8个不同摄像头，行人图像的边框由人工标注完成。
该数据集提供训练集和测试集。
训练集包含16,522张图像，测试集包含 17,661 张图像。
训练数据中一共有702人，平均每个人有23.5 张训练数据。
该数据集是目前最大的行人重识别数据集，并且提供了行人属性（性别/长短袖/是否背包等）的标注。

\item{VIPeR}

VIPeR \cite{Gray2007Evaluating}数据集是早期的一个小型行人重识别数据集，图像来自2个摄像头。
该数据集总共包含632个行人的1,264，每个行人有两张不同摄像头拍摄的图片。
数据集随机分为相等的两部分，一部分作为训练集，一部分作为测试集。
由于采集时间较早，该数据集的图像分辨率非常低，所以识别难度较大。

\item{PRID2011}

PRID2011 \cite{Hirzer2011Person}是2011年提出的一个数据集，图像来自于2个不同的摄像头。
该数据集总共包含934个行人的24,541张行人图片，所以的检测框都是人工手动提取。
图像大小的分辨率统一为$128 \times 64$的分辨率。
\end{itemize}

以上是目前行人重识别研究中主要运用的数据集。
由于行人重识别图片采自于不同摄像头，所以会出现光照、行人姿态、拍摄角度、遮挡、图像模糊等问题，造成同一行人的图片在不同摄像头中表现差异很大。
如图\ref{dataset_reid}所示，上一排与下一排为同一个行人在两个不同摄像头拍摄的图片。
可以看出，第一列存在遮挡现象，第二列至第四列存在拍摄角度、姿态等的巨大差异，第五列由于拍摄距离不同造成行人占图像比例大小差异很大，而最后一列是典型的摄像头分辨率不同而造成的图像差异。
正式因为各种因素造成的图像差异，所以使得行人重识别很难通过手动提取特征就达到很好的识别效果，需要通过一定手段来学习到非常鲁棒的图像特征。
\begin{figure}[htb]
\centering
\includegraphics[width=0.65\paperwidth]{./Pictures/dataset.jpg}
\caption{行人重识别数据集图片示例}
\label{dataset_reid}
\end{figure}


\subsection{准确度评估准则}

为了评估行人重识别算法的优劣，需要统一一些评价准则。
在学术论文中，通常大家默认选择累计匹配(Cumulative Match Characteristics, CMC)曲线和平均准确度(Mean Average Precision, mAP)来作为评价准则。
CMC和mAP是检索问题中常用的评价准则，在介绍它们之前，我们先介绍一些要用到常用术语。
\begin{itemize}
\item{query}：指测试集中的待检索库,包含图片的数目为$N_q$。

\item{gallery}：指测试集中的搜索库。

\item{probe}：指query中的某张待检索的图片，测试时需要将gallery中和probe为同一行人的图片全部检索出来。
\end{itemize}


\begin{enumerate}[(1)]
\item CMC曲线

CMC曲线主要用于计算rank-k的击中概率，在行人重识别、人脸识别领域使用较多。
针对于query集中的一张带检索的probe图片，返回gallery的一系列排好序的结果，排序按照相似度排序。
越靠前的结果表示和probe图片越相似，在行人重识别领域也等同于和probe是同一个人的概率越高。
在测试阶段，需要排除gallery集中和probe处于同一摄像头的图片，防止其参与检索排序。
我们设$index_{probe}$表示和gallery和probe为相同行人的最靠前的排序结果。
最后rank-k准确度$A(rank\text{-}k)$可以表示为：
\begin{equation}
\label{cmc}
A(rank\text{-}k) = \frac{\sum_{probe\in query}f_{CMC}(index_{probe},k)}{N_q}
\end{equation}
其中：
\begin{equation}
f_{CMC}(index_{probe},k)=\left\{
\begin{array}{rcl}
0       &      & {index_{probe} > k}\\
1       &      & {index_{probe} \leq k}
\end{array} \right.
\end{equation}
在实际使用中，为了减少计算量，通常我们比较关心rank-1,rank-5,rank-10,rank-20等准确度。

\item mAP

mAP是另外一种重要的评价指标。
CMC曲线通常只关心检索库中最靠前的正样本排序，而mAP由gallery中所有正样本的排序结果决定，所以通常能够更加鲁邦地反映模型的性能。
计算mAP需要以下三步：

（1）Precision：对于query中的某一张probe图片，返回了gallery的一系列排序结果，考虑前$n$ 个查询结果，$P(n)$=前$n$个结果中与probe图片是相同行人的数目/$n$；

（2）Average Precision：对于query的第$K$个probe图片，记录排序结果中所有M个正样本排序结果的集合$\{i_1,i_2,...,i_M\}$，计算它们的平均Precision，即$AP_K = \sum P(i)/M$，其中$i \in \{i_1,i_2,...,i_M\}$；

（3）Mean Average Precision (mAP)：所有$N_q$张probe图片的Average Precision 的平均值，即$mAP = \sum_K AP_K/N$。

\end{enumerate}

\subsection{基于表征学习的方法}

基于表征学习(Representation learning)的方法是一类非常常用的行人重识别方法\cite{geng2016deep,lin2017improving,zheng2016person,matsukawa2016person}。
这主要得益于深度学习，尤其是卷积神经网络(Convolutional neural network, CNN)\cite{Krizhevsky2012ImageNet}的快速发展。
由于CNN可以自动从原始的图像数据中根据任务需求自动提取出表征特征(Representation)，所以有些研究者把行人重识别问题看做分类(Classification/Identification)问题或者验证(Verification)问题。
分类问题是指利用行人的ID或者属性等作为训练标签来训练模型。
验证问题是指输入一对（两张）行人图片，让网络来学习这两张图片是否属于同一个行人。

论文\cite{geng2016deep}利用Classification/Identification loss和verification loss来训练网络，其网络示意图如图\ref{ident}所示。
网络输入为若干对行人图片，包括分类子网络(Classification Subnet)和验证子网络(Verification Subnet)。
分类子网络对图片进行ID预测，根据预测的ID来计算分类误差损失。
验证子网络融合两张图片的特征，判断这两张图片是否属于同一个行人，该子网络实质上等于一个二分类网络。
经过足够数据的训练，再次输入一张测试图片，网络将自动提取出一个特征，这个特征用于行人重识别任务。
\begin{figure}[hbt]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.65\paperwidth]{./Pictures/idnet.png}
  \caption{结合分类损失和验证损失训练ReID网络示意图}\label{ident}
\end{figure}

论文\cite{lin2017improving,zheng2016person,matsukawa2016person}认为光靠行人的ID信息不足以学习出一个泛化能力足够强的模型。
在这些工作中，它们额外标注了行人图片的属性特征，例如性别、头发、衣着等属性。
通过引入行人属性标签，模型不但要准确地预测出行人ID，还要预测出各项正确的行人属性，这大大增加了模型的泛化能力，多数论文也显示这种方法是有效的。
图\ref{attri}是其中一个示例，从图中可以看出，网络输出的特征不仅用于预测行人的ID信息，还用于预测各项行人属性。
通过结合ID损失和属性损失能够提高网络的泛化能力。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.65\paperwidth]{./Pictures/attri+id.jpg}
  \caption{结合行人ID标注和行人属性训练ReID网络示例}\label{attri}
\end{figure}

\subsection{基于度量学习的方法}

度量学习(Metric learning)是广泛用于图像检索利于的一种方法。
不同于表征学习，度量学习旨在通过网络学习出两张图片的相似度。
在行人重识别问题上，具体为同一行人的不同图片相似度大于不同行人的不同图片。
最后网络的损失函数使得相同行人图片（正样本对）的距离尽可能小，不同行人图片（负样本对）的距离尽可能大。
常用的度量学习损失方法有对比损失(Contrastive loss)\cite{varior2016gated}、三元组损失(Triplet loss)\cite{schroff2015facenet,liu2017end,cheng2016person}、 四元组损失(Quadruplet loss)\cite{chen2017beyond}。首先，假如有两张输入图片$I_1$和$I_2$，通过网络的前馈我们可以得到它们归一化后的特征向量$f_{I_1}$和$f_{I_2}$。我们定义这两张图片特征向量的欧式距离为：
\begin{equation}\label{dis}
  d_{I_1,I_2} = ||f_{I_1}-f_{I_2} ||_2
\end{equation}

\begin{enumerate}[(1)]
\item 对比损失(Contrastive loss)

对比损失用于训练孪生网络(Siamese network)，其结构图如图\ref{siamese}所示。
孪生网络的输入为一对（两张）图片$I_a$和$I_b$，这两张图片可以为同一行人，也可以为不同行人。
每一对训练图片都有一个标签$y$，其中$y=1$表示两张图片属于同一个行人（正样本对），反之$y=0$表示它们属于不同行人（负样本对）。
之后，对比损失函数写作:
\begin{equation}\label{contrastive}
  L_c = yd_{I_a,I_b}^2+(1-y)(\alpha - d_{I_a,I_b})^2_+
\end{equation}
其中$(z)_+$表示$max(z,0)$，$\alpha$是根据实际需求设计的阈值参数。
为了最小化损失函数，当网络输入一对正样本对，$d(I_a,I_b)$会逐渐变小，即相同ID的行人图片会逐渐在特征空间形成聚类。
反之，当网络输入一对负样本对时，$d(I_a,I_b)$会逐渐变大直到超过设定的$\alpha$。
通过最小化$L_c$，最后可以使得正样本对之间的距离逐渐变下，负样本对之间的距离逐渐变大，从而满足行人重识别任务的需要。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.65\paperwidth]{./Pictures/siamese.png}
  \caption{孪生网络结构示意图}\label{siamese}
\end{figure}

\item 三元组损失(Triplet loss)

三元组损失是一种被广泛应用的度量学习损失，之后的大量度量学习方法也是基于三元组损失演变而来。
顾名思义，三元组损失需要三张输入图片。
和对比损失不同，一个输入的三元组（Triplet）包括一对正样本对和一对负样本对。
三张图片分别命名为固定图片(Anchor) $a$，正样本图片(Positive) $p$和负样本图片(Negative) $n$。
图片$a$和图片$p$为一对正样本对，图片$a$和图片$n$为一对负样本对。
则三元组损失表示为：
\begin{equation}\label{Triplet}
  L_t = (d_{a,p}-d_{a,n}+\alpha)_+
\end{equation}

如图\ref{triplet}所示，三元组可以拉近正样本对之间的距离，推开负样本对之间的距离，最后使得相同ID的行人图片在特征空间里形成聚类，达到行人重识别的目的。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\paperwidth]{./Pictures/triplet.png}
  \caption{三元组损失\cite{Liu2016Deep}}\label{triplet}
\end{figure}

论文\cite{cheng2016person}认为公式\eqref{Triplet}只考虑正负样本对之间的相对距离，而并没有考虑正样本对之间的绝对距离，为此提出改进三元组损失(Improved triplet loss)：
\begin{equation}\label{ImTriplet}
  L_it = d_{a,p}+(d_{a,p}-d_{a,n}+\alpha)_+
\end{equation}
公式\eqref{ImTriplet}添加$d_{a,p}$项，保证网络不仅能够在特征空间把正负样本推开，也能保证正样本对之间的距离很近。

\item 四元组损失(Quadruplet loss)

四元组损失是三元组损失的另一个改进版本。
顾名思义，四元组(Quadruplet)需要四张输入图片，和三元组不同的是多了一张负样本图片。
即四张图片为固定图片(Anchor) $a$，正样本图片(Positive) $p$,负样本图片1(Negative1) $n1$ 和负样本图片2(Negative2) $n2$。
其中$n1$和$n2$是两张不同行人ID的图片，其结构如图\ref{quadruplet}所示。
则，四元组损失表示为：

\begin{equation}\label{Quadruplet}
  L_q = (d_{a,p}-d_{a,n1}+\alpha)_+ + (d_{a,p}-d_{n1,n2}+\beta)_+
\end{equation}
其中$\alpha$和$\beta$是手动设置的正常数，通常设置$\beta$小于$\alpha$，前一项称为强推动，后一项称为弱推动。
相比于三元组损失只考虑正负样本间的相对距离，四元组添加的第二项不共享ID，所以考虑的是正负样本间的绝对距离。
因此，四元组损失通常能让模型学习到更好的表征。

\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.65\paperwidth]{./Pictures/quadruplet.png}
  \caption{四元组损失网络结构图\cite{chen2017beyond}}\label{quadruplet}
\end{figure}

\item{难样本采样三元组损失(Triplet loss with hard sample mining)}

难样采样三元组损失（本文之后用TriHard损失表示）是三元组损失的改进版。
传统的三元组随机从训练数据中抽样三张图片，这样的做法虽然比较简单，但是抽样出来的大部分都是简单易区分的样本对。
如果大量训练的样本对都是简单的样本对，那么这是不利于网络学习到更好的表征。
大量论文发现用更难的样本去训练网络能够提高网络的泛化能力，而采样难样本对的方法很多。
论文\cite{hermans2017defense}提出了一种基于训练批量(Batch)的在线难样本采样方法――TriHard损失。

TriHard损失的核心思想是：对于每一个训练batch，随机挑选$P$个ID的行人，每个行人随机挑选$K$ 张不同的图片，即一个batch含有$P \times K$张图片。
之后对于batch中的每一张图片$a$，我们可以挑选一个最难的正样本和一个最难的负样本和$a$组成一个三元组。

首先我们定义和$a$为相同ID的图片集为$A$，剩下不同ID的图片图片集为$B$，则TriHard损失表示为：
\begin{equation}\label{TriHard}
  L_{th} = \frac{1}{P \times K}\sum_{a \in batch}(\max_{p \in A} d_{a,p}-\min_{n \in B} d_{a,n}+\alpha)_+
\end{equation}
其中$\alpha$是人为设定的阈值参数。
TriHard损失会计算$a$和batch中的每一张图片在特征空间的欧式距离，然后选出与$a$ 距离最远（最不像）的正样本$p$和距离最近（最像）的负样本$n$来计算三元组损失。
通常TriHard损失效果比传统的三元组损失要好。

\end{enumerate}

\subsection{基于局部特征的方法}
从网络的训练损失函数上进行分类可以分成表征学习和度量学习，相关方法前文已经介绍。
另一个角度，从抽取图像特征进行分类，行人重识别的方法可以分为基于全局特征(Global feature) 和基于局部特征(Local feature)的方法。
全局特征是指让网络对整幅图像提取一个特征，这个特征不考虑一些局部信息。
而局部特征是指让手动或者自动地让网络去关注关键的局部区域，然后提取这些区域的局部特征。
常用的提取局部特征的思路主要有图像切块、利用骨架关键点定位以及姿态矫正等等。

图片切块是一种很常见的提取局部特征方式\cite{xiao2016cross,varior2016siamese}。
如图\ref{local1}所示，图片被垂直等分为若干份，因为垂直切割更符合我们对人体识别的直观感受，所以行人重识别领域很少用到水平切割。
之后，被分割好的若干块图像块按照顺序送到一个长短时记忆网络(Long short term memory network, LSTM)，最后的特征融合了所有图像块的局部特征。
但是这种缺点在于对图像对齐的要求比较高，如果两幅图像没有上下对齐，那么很可能出现头和上身对比的现象，反而使得模型判断错误。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.7\paperwidth]{./Pictures/local1.png}
  \caption{利用图片切块提取局部特征示例\cite{varior2016siamese}}\label{local1}
\end{figure}

为了解决图像不对齐情况下手动图像切片失效的问题，一些论文利用一些先验知识先将行人进行对齐，这些先验知识主要是预训练的人体姿态(Pose)和骨架关键点(Skeleton) 模型。
论文\cite{zheng2017pose}先用姿态估计的模型估计出行人的关键点，然后用仿射变换使得相同的关键点对齐。
如图\ref{pose}所示，一个行人通常被分为14个关键点，这14个关键点把人体结果分为若干个区域。
为了提取不同尺度上的局部特征，作者设定了三个不同的PoseBox组合。
之后这三个PoseBox矫正后的图片和原始为矫正的图片一起送到网络里去提取特征，这个特征包含了全局信息和局部信息。
特别提出，如果这个仿射变换可以在进入网络之前的预处理中进行，也可以在输入到网络后进行。
如果是后者的话需要需要对仿射变换做一个改进，因为传统的放射变化是不可导的。
为了使得网络可以训练，需要引入可导的近似放射变化，在本文中不赘述相关知识。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\paperwidth]{./Pictures/pose.png}
  \caption{姿态对齐示意图\cite{zheng2017pose}}\label{pose}
\end{figure}

CVPR2017的工作Spindle Net\cite{zhao2017spindle}也利用了14个人体关键点来提取局部特征。
和论文\cite{zheng2017pose}不同的是，Spindle Net并没有用仿射变换来对齐局部图像区域，而是直接利用这些关键点来抠出感兴趣区域(Region of interest, ROI)。
Spindle Net网络如图\ref{spindle}所示，首先通过骨架关键点提取的网络提取14个人体关键点，之后利用这些关键点提取7个人体结构ROI。
网络中所有提取特征的CNN（橙色表示）参数都是共享的，这个CNN分成了线性的三个子网络FEN-C1、FEN-C2、FEN-C3。
对于输入的一张行人图片，有一个预训练好的骨架关键点提取CNN（蓝色表示）来获得14个人体关键点，从而得到7个ROI区域，其中包括三个大区域（头、上身、下身）和四个四肢小区域。
这7个ROI区域和原始图片进入同一个CNN网络提取特征。
原始图片经过完整的CNN得到一个全局特征。
三个大区域经过FEN-C2和FEN-C3子网络得到三个局部特征。
四个四肢区域经过FEN-C3子网络得到四个局部特征。
之后这8个特征按照图示的方式在不同的尺度进行联结，最终得到一个融合全局特征和多个尺度局部特征的行人重识别特征。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\paperwidth]{./Pictures/spindle.png}
  \caption{Spindle Net结构示意图\cite{zhao2017spindle}}\label{spindle}
\end{figure}

论文\cite{wei2017glad}提出了一种全局-局部对齐特征描述子(Global-Local-Alignment Descriptor, GLAD)，来解决行人姿态变化的问题。
与Spindle Net类似，GLAD利用提取的人体关键点把图片分为头部、上身和下身三个部分。
之后将整图和三个局部图片一起输入到一个参数共享CNN网络中，最后提取的特征融合了全局和局部的特征。
为了适应不同分辨率大小的图片输入，网络利用全局平均池化(Global average pooling, GAP)来提取各自的特征。
和Spindle Net略微不同的是四个输入图片各自计算对应的损失，而不是融合为一个特征计算一个总的损失。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.65\paperwidth]{./Pictures/glad.png}
  \caption{GLAD结构示意图\cite{wei2017glad}}\label{glad}
\end{figure}


\subsection{基于视频序列的方法}

以上介绍的方法都是基于单帧图像的方法，通常单帧图像的信息是有限的，因此有很多工作集中在利用视频序列来进行行人重识别方法的研究\cite{wang2016person, zhang2017image, you2016top, ma2017person, mclaughlin2016recurrent, zhao2017person, liu2017video}。基于视频序列的方法最主要的不同点就是这类方法不仅考虑了图像的内容信息，还考虑了帧与帧之间的运动信息等。

基于单帧图像的方法主要思想是利用CNN来提取图像的空间特征，而基于视频序列的方法主要思想是利用CNN 来提取空间特征的同时利用递归循环网络(Recurrent neural networks, RNN)来提取时序特征。
图\ref{video_reid}是非常典型的思路，网络输入为图像序列。
每张图像都经过一个共享的CNN提取出图像空间内容特征，之后这些特征向量被输入到一个RNN网络去提取最终的特征。
最终的特征融合了单帧图像的内容特征和帧与帧之间的运动特征。
而这个特征用于代替前面单帧方法的图像特征来训练网络。


\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\paperwidth]{./Pictures/video_reid.png}
  \caption{基于视频序列的行人重识别网络结构示意图}\label{video_reid}
\end{figure}

视频序列类的代表方法之一是累计运动背景网络(Accumulative motion context network, AMOC)\cite{liu2017video}。
AMOC输入的包括原始的图像序列和提取的光流序列。
通常提取光流信息需要用到传统的光流提取算法，但是这些算法计算耗时，并且无法与深度学习网络兼容。
为了能够得到一个自动提取光流的网络，作者首先训练了一个运动信息网络(Motion network, Moti Nets)。
这个运动网络输入为原始的图像序列，标签为传统方法提取的光流序列。
如图\ref{motion}所示，原始的图像序列显示在第一排，提取的光流序列显示在第二排。
网络有三个光流预测的输出，分别为Pred1，Pred2，Pred3，这三个输出能够预测三个不同尺度的光流图。
最后网络融合了三个尺度上的光流预测输出来得到最终光流图，预测的光流序列在第三排显示。
通过最小化预测光流图和提取光流图的误差，网络能够提取出较准确的运动特征。

\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.6\paperwidth]{./Pictures/video_flow.png}
  \caption{运动网络结构示意图\cite{liu2017video}}\label{motion}
\end{figure}

AMOC的核心思想在于网络除了要提取序列图像的特征，还要提取运动光流的运动特征，其网络结构图如图\ref{AMOC}所示。
AMOC拥有空间信息网络(Spatial network, Spat Nets)和运动信息网络两个子网络。
图像序列的每一帧图像都被输入到Spat Nets来提取图像的全局内容特征。
而相邻的两帧将会送到Moti Nets来提取光流图特征。
之后空间特征和光流特征融合后输入到一个RNN来提取时序特征。
通过AMOC网络，每个图像序列都能被提取出一个融合了内容信息、运动信息的特征。
网络采用了分类损失和对比损失来训练模型。
融合了运动信息的序列图像特征能够提高行人重识别的准确度。

\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.6\paperwidth]{./Pictures/AMOC.png}
  \caption{AMOC结构示意图\cite{liu2017video}}\label{AMOC}
\end{figure}


\section{跨摄像头多目标跟踪}

\subsection{相关数据集}

\begin{enumerate}[(1)]

\item DukeMTMC
DukeMTMC\cite{Ristani2016Performance}是杜克大学五位博士研究生花费一年多时间标注的一个跨摄像头多目标跟踪的数据集，是目前最好最新的MTMC数据集。
该数据集总共包括8个固定摄像头拍摄的85分钟视频数据，视频为60fps的1080p分辨率图像。
总共人工标注了2,000,000帧图像数据，其中超过2,000名行人，比目前所有的MTMC数据集加起来还多。
所有的跟踪序列加起来时长超过了30多个小时。
针对单个摄像头，单帧图像包含行人数最少0人，最多54人。
总共有4,159次轨迹的切换以及50个轨迹线的盲点，此外还有1,800自我遮挡发生。
有两对相机(2-8,3-5)的视野有微量的重合，所以这个既可以用来研究有重合的跨摄像头跟踪，也可以用来研究无重合的跨摄像头跟踪。
每个摄像头前5分钟的视频用来作为训练集或者验证集，剩下的80分钟用来作为测试集。
总共有891个行人只在一个摄像头中出现过，跟踪器很容易产生$FP$，这对于跟踪器也是一个非常大的考验。
DukeMTMC数据集的数据示例如图\ref{dukemtmc}所示。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.7\paperwidth]{./Pictures/dukemtmc.jpg}
  \caption{DukeMTMC数据集示意图\cite{Ristani2016Performance}}\label{dukemtmc}
\end{figure}
\begin{figure}[b]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\paperwidth]{./Pictures/dukemtmc1.png}
  \caption{DukeMTMC数据集世界坐标示意图\cite{Ristani2016Performance}}\label{dukemtmc1}
\end{figure}

当然除了标注行人的ID和跟踪序列，考虑到时空信息关联的问题，DukeMTMC数据集还给定了8个摄像头的相机标定数据，既提供相机内参也提供相机外参数据。
这些相机标定数据使得研究人员可以得到跟踪目标在世界坐标系下的时空坐标，这个信息对于滤波相关的方法是非常有利的。
利用相机标定数据重建的8个相机的视野在杜克大学校园地图上的投影分布如图\ref{dukemtmc1}。

\item MOT16

MOT16\cite{Milan2016MOT16}是一个单摄像头的多目标跟踪数据集，总共包括14个视频数据，其中7个训练数据，剩下7个为测试数据。
这些视频来自于7个不同的场景，每个场景的视频都是随机切成完成不重合的两部分，分别作为训练和测试数据。
和DukeMTMC不同的是，DukeMTMC只关注行人目标，而MOT数据集主要关注行人目标，其次还关注、汽车、自行车、摩托车等12种常见目标。
由于视频的来源各自不同，所以视频的分辨率和帧率等属性也各自不同，具体属性数据如图\ref{MOT16_attri}所示。
可以看出，视频既有固定视角拍摄的，也有手持移动拍摄，总体来讲是一个非常多样性、非常具有挑战性的多目标跟踪数据集。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.75\paperwidth]{./Pictures/MOT16_attri.png}
  \caption{MOT16数据集各视频属性\cite{Milan2016MOT16}}\label{MOT16_attri}
\end{figure}

MOT16数据集标注是十分精确了，每个目标的检测框都对齐的足够精细了，几乎没有漏框目标的任何一个像素也没有额外消耗多余的像素，也就是基本框住了目标的轮廓边界。
最后MOT16数据集总共标注了215,166个检测框，平均每帧有19.15个检测框，即每帧平均有19.15个跟踪目标。
其中有一大半是普通的行人，因此可以用于本课题的研究。
MOT16数据集的示例如图\ref{MOT16}所示，总共有7个场景的14个视频序列，上面的为训练集，下面的为测试集。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.7\paperwidth]{./Pictures/MOT16.png}
  \caption{MOT16数据集各视频属性}\label{MOT16}
\end{figure}
\item PETS16

PETS16\cite{Patino2016PETS}是一个多目标跟踪的数据集，因为和本课题研究的应用场景没有非常契合，因此简单介绍一下。
该数据集总共包括14个视频数据，其中6个为训练集，8个为测试集。
视频分为480p、512p、960p和1280p四种分辨率，帧率为25fps或者30fps两种规格。
训练集总共标注了7,051个检测框，测试集标注8,025个检测框。
该数据集主要提供道路上行走的行人和水面上高速移动的船只的跟踪序列标注，拍摄的视角基本都是低空的摄像头，比如从卡车副驾驶座拍摄的。
数据示例如图\ref{PETS}所示。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.7\paperwidth]{./Pictures/PETS.png}
  \caption{MOT16数据集各视频属性\cite{Patino2016PETS}}\label{PETS}
\end{figure}

\end{enumerate}

\clearpage
\subsection{准确度评估准则}

跨摄像头多目标跟踪准确度的评价准则有很多，主要包括单摄像头的多目标跟踪和跨摄像头两部分，本小节将逐个介绍\cite{Ristani2016Performance,Keni2008Evaluating,Milan2013Challenges}。

\begin{itemize}
\item{TP}：真正(True Positive, TP)是指被模型预测为正的正样本，可以称为判断为正的正确率。
\item{TN}：真负(True Negative, TN)是指被模型预测为负的负样本，可以称为判断为负的正确率。
\item{FP}：假正(False Positive, FP)是指被模型预测为正的负样本，可以称为误报率。
\item{FN}：假负(FALSE Negative, FN)是指被模型预测为负的正样本，可以称为漏报率。
\item{Accuracy}：准确度是指被分类器判定正确的比重，公示表示为：
\begin{equation}\label{Accuracy}
  A = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}
\item{Precision}：精确度是指被分类器判定的正例中真正的正例样本的比重，公示表示为：
\begin{equation}\label{Precision}
  P = \frac{TP}{TP+FP}
\end{equation}
\item{Recall}：召回率是指被分类器正确判定的正例占总的正例的比重，公示表示为：
\begin{equation}\label{Recall}
  R = \frac{TP}{TP+FN}
\end{equation}
\item{MOTA}：多目标跟踪准确度(Multiple Object Tracking Accuracy, MOTA)是衡量单摄像头多目标跟踪准确度的一个指标，公示表示为：
    \begin{equation}\label{MOTA}
      MOTA = 1 - \frac{FN+FP+\mathit{\Phi}}{T}
    \end{equation}
    其中$FN$是指所有帧的假负数之和，即假设$fn_t$为第$t$帧的假负数，则$FN = \sum_t fn_t$，同理$FP = \sum_t fp_t$。
    $T$是指所有帧真正目标数的总和，即假设第$t$帧有$g_t$个目标，则$T = \sum_t g_t$。
    $\mathit{\Phi}$是指所有帧目标发生跳变数(Fragmentation)，$\phi_t$为第$t$帧的目标跳变数，则$\mathit{\Phi} = \sum_t \phi_t$。换而言之，这三项依次表示缺失率、误判率和误配率。
\item{MOTP}：多目标跟踪精确度(Multiple Object Tracking Precision, MOTP)是衡量单摄像头多目标跟踪位置误差的一个指标，公式表示为：
    \begin{equation}\label{MOTP}
      MOTP = \frac{\sum_{i,t}d_t^i}{\sum_t c_t}
    \end{equation}
    其中$c_t$表示第$t$帧的匹配个数，对每对匹配计算匹配误差$d_t^i$，表示第$t$帧下目标$O_i$与其配对假设位置之间的距离。

\item{MT}：多数跟踪数(Mostly tracked)是指跟踪部分大于80$\%$的跟踪轨迹数，数值越大越好。

\item{ML}：多数丢失数(Mostly lost)是指丢失部分大于80$\%$的跟踪轨迹数，数值越小越好。

\item{IDS}：ID转变数(ID switches)是指跟踪轨迹中行人ID瞬间转换的次数，通常能反应跟踪的稳定性，数值越小越好。

\item{IDP}：识别精确度(Identification Precision)是指每个行人框中行人ID识别的精确度。公式为：
    \begin{equation}\label{IDP}
      IDP = \frac{IDTP}{IDTP+IDFP}
    \end{equation}
    其中$IDTP$和$IDFP$分别是真正ID数和假正ID数。

\item{IDR}：识别回召率(Identification Recall)是指每个行人框中行人ID识别的回召率。公式为：
    \begin{equation}\label{IDR}
      IDP = \frac{IDTP}{IDTP+IDFN}
    \end{equation}
    其中$IDFN$是假负ID数。

\item{IDF1}：识别F值(Identification F-Score)是指每个行人框中行人ID识别的F 值。公式为：
    \begin{equation}\label{IDR}
      IDP = \frac{2IDTP}{2IDTP+IDFP+IDFN}
    \end{equation}

\item{MCTA}：跨摄像头跟踪准确度(Multi-camera Tracking Accuracy)是衡量多个摄像头下跟踪的准确度，是目前少有的专门用来衡量多摄像头跟踪性能的评价指标。公式为：
    \begin{equation}\label{MCTA}
      MCTA = \underbrace{\frac{2PR}{P+R}}_{F1}\underbrace{(1-\frac{M^w}{T^w})}_{\text{within camera}}\underbrace{(1-\frac{M^h}{T^h})}_{\text{handover}}
    \end{equation}
    其中$P,R$为前文介绍的精准度和回召率，$M^w$是单相机内行人ID的错误匹配数，$T^w$是单相机内（标注）正确检测数，$M^h$是跨相机行人ID的错误匹配数，$T^h$是指跨相机（标注的）正确检测数（即某个目标从某个相机中消失而下次再出现在另外一个相机的情况）。MCTA的范围是$[0,1]$。
\end{itemize}

\subsection{表观模型}
表观模型(Appearance model)既包括目标的视觉特征表达，也包括目标间相似性、相异性的度量。
视觉表达肯定是基于图像特征的，在深度学习出现之前，学者们通常手动提取一些传统特征，由于在本课题研究中主要使用行人重识别的深度学习特征，所以在这里只是简单提及一下这些传统特征。
传统的图像特征包括：
\begin{itemize}
  \item Point feature，比如Harris角点、SIFT角点、SURF角点等等
  \item Color/intensity features，比如最简单的模板、颜色直方图等
  \item Optical flow，光流特征蕴含了时域信息
  \item Gradient/pixel-comparison features，基于梯度的特征，典型的如HOG特征
  \item Region covariance matrix features，该特征对于光照和尺度变换相对鲁棒
  \item Depth，即深度信息，对于视频这种3D数据作用还是蛮大的
\end{itemize}
这些图像特征在传统的多目标跟踪中有非常广泛的应用，但是随着深度学习和行人重识别的发展，ReID特征逐渐成为一种优秀的表观模型\cite{Beyer2017Towards,Leal2016Learning}。

论文\cite{Beyer2017Towards}结合ReID特征和其他一些数据关联(Data association, DA)，给定两个检测框的图片$I_1$和$I_2$，它们的距离（和相似度呈反比）公示表示如下：
\begin{equation}\label{DA}
  d(I_1,I_2) = \frac{d_{pos}(I_1,I_2)}{N_{pos}}\frac{d_{app}(I_1,I_2)}{N_{app}}
\end{equation}
其中$d_{app}$是两张图片的ReID特征的距离，比如最常用的欧式距离。
$N_{pos}$和$N_{app}$是归一化的参数，使得$d_{pos}$和$d_{app}$能够处于相同的数量级。
$d_{pos}$是一些传统数据关联方法，这里可以被任何相关的方法所取代。

论文\cite{Leal2016Learning}则更加直接，通过训练一个孪生网络来判断两个detection框是否是需要匹配。
如图\ref{Siamese_mtt}所示，检测器检测出若干个detections，之后通过一个已经训练好的行人重识别孪生网络，把相同的小段轨迹(Tracklet)关联起来，之后通过一个线性规划(Linear Programming)方法得到最终的跟踪轨迹(Trajectory)。
这种方法比较简单，只需要一个检测器和ReID模型便可以实现MTMC跟踪问题，是业界非常常用的一种方法。
但是这种方法的缺点也很明显，非常依赖检测器和ReID模型的性能。
在本课题中我们不关注检测器的研究，即是在有一个很好的检测器的前提下开展MTMC工作，那么这种方法最后的性能完全由ReID模型的影响。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\paperwidth]{./Pictures/siamese_mtt.png}
  \caption{基于行人重识别的孪生网络实现多目标跟踪\cite{Leal2016Learning}}\label{Siamese_mtt}
\end{figure}

\subsection{基于相关滤波的多目标跟踪}

相关滤波是一类非常常见的目标跟踪方法，贝叶斯滤波(Bayesian filter)、卡尔曼滤波(Kalman filter)、 粒子滤波(Particle filter) 等都在目标跟踪问题上有所应用。
然而在跨摄像头多目标跟踪问题上，本来相关工作就不多，和行人重识别结合起来的工作就更加少之又少。
这里介绍一篇结合ReID特征和贝叶斯滤波的MTMC论文\cite{Beyer2017Towards}。

给定一个ReID模型$f_{\theta}$，图片$I_p$可以得到一个嵌入特征$e_p = f_{\theta}(I_p)$。
对于完整的某一帧图像$I$，可以得到一个$D_I(e_p) = (||e_{i,j}-e_p||)_{i,j}$，$e_{i,j}$代表图像$I$该位置中心的图像切片与目标图片的ReID特征之间的距离，距离越小代表越相似。
最后可以得到一个嵌入特征距离图(Embedding distance map)$D_I$。
之后这个$D_I$可以转化为一个贝叶斯滤波器的观测，观测模型表示为：
\begin{equation}\label{Measurement}
  P(z_t|X_t,z_{1:t-1})=softmin(D_I(f(Xt,z_{1:t-1})))
\end{equation}
其中$e_p=f(Xt,z_{1:t-1})$可以通过很多方式更新，在论文中就简单用第一次出现的代替并不进行更新，即$e_p=f_{\theta}(z_1)$ 。
当然为了适应这个新的观测器模型，我们需要对传统的最优贝叶斯滤波进行一个重构，重构利用概率的贝叶斯规则，最终表达如下：
\begin{equation}\label{Measure2}
  P(X_t|z_t,z_{1:t-1}) \propto \overbrace{P(z_t|X_t,z_{1:t-1})}^{\text{new measurement}}\overbrace{P(X_t|z_{1:t-1})}^{\text{belief propagation}}
\end{equation}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\paperwidth]{./Pictures/bayes.png}
  \caption{基于行人重识别和贝叶斯滤波的多目标跟踪\cite{Beyer2017Towards}}\label{Bayes}
\end{figure}
公示被分解为两个部分，前一部分是当前最新的观测结果，后一部分可以用前一时刻的状态进行状态估计。
当我们有了观测模型之后，下一步就是用贝叶斯滤波器进行状态估计。
公示\eqref{Measure2}的后面一项可以利用全概率模型和马尔科夫规则进行进一步的分解：
\begin{equation}\label{Predict}
 P(X_t|z_{1:t-1}) = \int {\overbrace{P(X_t|x_{t-1})}^{\text{dynamics model}}P(x_{t-1}|z_{1:t-1})} dx_{t-1}
\end{equation}
这个公示可以用上一个时刻的状态$X_{t-1}$和所有时刻的观测$z_{1:t-1}$估计当前时刻的状态$X_t$。
$P(X_t|x_{t-1})$代表动态模型，比较典型的例如MTMC中的运动模型中的行人恒速约束。
随着一个新的观测$z_t$输入，可以根据贝叶斯规则更新后验概率：
\begin{equation}\label{posterior}
  P(X_t|z_{1:t})=\frac{\overbrace{P(z_t|X_t,z_{1:t-1})}^{\text{measurement model}}P(X_t|z_{1:t-1})}{\int {P(z_t|x_t,z_{1:t-1})P(x_t|z_{1:t-1})}d_{x_t}}
\end{equation}
这个和公示\eqref{Measurement}是相关的，特别提出当没有观测结果的时候，$P(zt|X_t,z_{1:t-1})$是均匀分布，因此在归一化之后可以抵消。
也就是说后验估计等于先验概率：$P(X_t|z_t)=P(X_t|z_{1:t-1})$。

在跟踪问题中，状态包括位置、速度、加速度、外观已经检测框的bounding boxes和提供的位置信息等等。
对于多目标跟踪问题有两种方法实现：一种是把所有人的状态作为一个观测，最终只使用一个贝叶斯滤波器；一种是对每一个人都作为一个观测，每一个人都有自己的贝叶斯滤波器。
根据这个贝叶斯滤波器的理论，再使用常见的一些数据关联方法，就可以实现一个MTMC 的跟踪系统。
为了进一步简化，也可以在当前的观测引入马尔科夫假设，即：$P(z_t|X_t,z_{1:t-1}) = P(z_t|X_t)$。
这就是一个基于相关滤波算法的MTMC系统的示例，当然卡尔曼滤波和粒子滤波也是应用的，大体思路和贝叶斯滤波相似，通过观测来进行状态估计，然后依靠观测和估计信息来更新模型，在本文不在赘述。


\subsection{基于代价函数的多目标跟踪}

基于代价函数(Cost function)是另一种常见的多目标跟踪方法，这类方法是通过设计一个合理的代价函数，通过最小化代价函数来实现跟踪轨迹的关联。
大量工作通过手动提取特征并精心设计这个代价函数来实现多目标跟踪，例如论文\cite{Leal2012Everybody,Milan2013Continuous} 依赖检测框的置信度和空间时序距离来设计这个代价函数。
论文\cite{Zamir2012GMCP}则考虑包括颜色直方图在内的一些外观特征也赋值进行数据关联。
为了解决更长时间的跟踪序列关联问题，论文\cite{Li2009Learning}设计了一个多层次的关联模型。
基于ReID特征和代价函数的多目标跟踪方法比较主流的做法是，把ReID特征作为一个外观模型，结合其他的一些状态信息，例如时空位置、移动速度等，计算轨迹与轨迹之间的关系矩阵。
然后设计一个合理的代价函数，利用一些优化算法来最小这个代价函数，把跟踪序列关联起来得到最终各个目标的跟踪轨迹。

\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.75\paperwidth]{./Pictures/DNF.png}
  \caption{利用网络流图进行3帧图像多目标跟踪的示例\cite{Schulter2017Deep}}\label{DNF}
\end{figure}

这里介绍CVPR2017的一篇工作\cite{Schulter2017Deep}，该论文利用图论的思想，构建一个网络流的代价函数，来得到跟踪轨迹。
如图\ref{DNF}，图中两个节点中间的红线代表检测框$d_i$，这根连线赋值流变量$x_i^{det}$。
对于两个检测框$d_i$（出节点）和$d_j$（入节点）满足$t(d_i)<t(d_j)$和$|t(d_i)-t(d_j)|<\tau_t$，图中蓝色箭头就连接属于同一个轨迹$\tau$的这两个检测框。
这些线赋值为$x_{i,j}^{link}$。
连线不一定需要是相邻帧的，也可以是跨越多帧来处理遮挡或者检测丢失的情况。
为了降低图的大小，对连线的空间进行了一个限制，即空间位置相差很远的两个检测框不会被连接。
$S$和$T$是轨迹的起点和终点，分别用$x_i^{in}$和$x_i^{out}$表示，在图中用黑线和其他节点连接。
图中的每一个变量都对应了一个代价，四种变量类型分别对应了$c^{in},c^{out},c^{det}$和$c^{link}$。
之后，问题可以表示为优化一下代价函数：
\begin{align}\label{cost}
  x^{*}& = \arg min c^Tx \\
  & s.t. \quad  Ax \leq b,Cx =0
\end{align}
其中$x \in \mathbb{R}^M$和$c \in \mathbb{R}^M$是分别是所有的连接和代价，$M$ 是问题的维度。
理论上$x$应该为实整数，为了简化把这个约束放宽到$0 \leq x \leq 1$。
公示\eqref{cost}中的方程系数为$A = [I,-I]^T \in \mathbb{R}^{2M \times M}$和$b = [1,0]^T \in \mathbb{R}^{2M}$。
根据流守恒约束，对于$\forall i$有$x_i^{in}+\sum_j x_{ji}^{link} = x_i^{det}$ 和$x_i^{out}+\sum_j x_{ji}^{link} = x_i^{det}$，而且$C\in \mathbb{R}^{2K \times M}$，其中$K$是检测框的数目。

之后我们把代价函数用$c(f,\mathbf{\theta})$表示，其中$\mathbf{\theta}$是待学习的参数，$f$是输入的数据，对于多目标跟踪问题，输入数据包括检测框的坐标、检测框的置信度、图像特征或者一些其他特征等等。
给定一些已经标注好跟踪序列的训练数据，最终的目标就是为了学习出一组参数$\mathbf{\theta}$，来使得代价函数最小。
因此问题可以转化为一个优化问题：

\begin{align}\label{eq}
  \mathop{\arg\min}_{\mathbf{\theta}} & \mathit{L}(x^{gt},x^{*})\\
  s.t. \quad &x^{*} = \mathop{\arg\min}_{x} c(f,\mathbf{\theta})^Tx \\
  & Ax \leq b,Cx = 0
\end{align}
最终的目标就是最小化损失函数$\mathit{L}$，通过对目标函数进行一些低阶近似，便可以求出目标函数的导师，来通过梯度下降的方法求解，具体细节可以查阅原论文，在此不做赘述。

\subsection{跨摄像头匹配}

跨摄像头多目标跟踪和多目标跟踪最大的区别就是多了一个跨摄像头匹配的环节，只要解决跨摄像头匹配的问题，那么任何一个多目标跟踪算法都可以扩展为跨摄像头多目标跟踪算法。
然而跨摄像头匹配这个问题是个十分难的问题，除了直接采用行人重识别来解决，相关的工作非常少，这里介绍一篇利用图论来解决该问题的工作\cite{Tesfaye2017Multi}。

给定一个跟踪轨迹集合$T$，其中$T_i^j$表示第$i$个相机中第$j$个跟踪轨迹。
之后我们可以构造一个图$G^{\prime}(V^{\prime},E^{\prime},w^{\prime})$，图中的每个节点代表一个跟踪轨迹。
假设我们有$\mathcal{I}$个相机，$A^{i\times j}$表示相机$i$和相机$j$之间所有跟踪轨迹的相似度，最终我们可以得到一个相似度矩阵：
\begin{equation*}
A=\begin{bmatrix}
          A^{1\times 1} & \cdots & A^{1\times j} & \cdots & A^{1 \times \mathcal{I}} \\
          \vdots & \ddots & \vdots & \ddots & \vdots \\
          A^{i\times 1} & \cdots & A^{i\times j} & \cdots & A^{i\times \mathcal{I}} \\
          \vdots & \ddots & \vdots & \ddots & \vdots \\
          A^{\mathcal{I}\times 1} & \cdots & A^{\mathcal{I}\times j} & \cdots & A^{\mathcal{I}\times \mathcal{I}}
\end{bmatrix}
\end{equation*}

如图\ref{Across}所示，节点的颜色代表行人的ID，黑色线是单相机内的轨迹匹配，彩色线是跨相机的轨迹匹配。
我们假设相机1包含的轨迹集$\mathcal{Q} = \{T_1^1,T_1^2,T_1^i,T_1^p \}$。
$I_{\mathcal{Q}}$是一个$n \times n$的对角阵，对角元素都是1。
$C_i^j$表示用第$j$个相机的第$i$个跟踪轨迹作为约束集而产生的轨迹集合，而$C_j$ 是指用第$j$个相机中的轨迹作为约束集而产生的集合，举个例子$C_1 = \{C_1^1,C_1^2,C_1^3\}$。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.6\paperwidth]{./Pictures/across.png}
  \caption{跨摄像头匹配示例\cite{Tesfaye2017Multi}}\label{Across}
\end{figure}


最终匹配的伪代码如图\ref{Algorithm}所示。
其中$\mathcal{T}$表示所有跟踪轨迹的集合，$C$表示所有轨迹集合的集合，$T_p$表示第$p$个相机中所有的跟踪轨迹集合。
函数$\mathcal{F}(\mathcal{Q},A)$的输入时约束集$\mathcal{Q}$和关系矩阵$A$，输出一个$m$局部解$\mathcal{X^{n \times m}}$。
之后根据这个结果更新各个集合参数，直到最后所有的跟踪轨迹都得到聚类。
具体的算法细节可以查阅原文献。
\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\paperwidth]{./Pictures/algorithm.png}
  \caption{跨摄像头匹配算法伪代码\cite{Tesfaye2017Multi}}\label{Algorithm}
\end{figure}
