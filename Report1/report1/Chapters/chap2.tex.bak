\chapter{卷积神经网络}
卷积神经网络是深度学习的标志性成果，其前身是神经网络。
2012年，Hinton团队首次利用卷积神经网络Alexnet获得ImageNet挑战赛的冠军，并大幅提高识别准确度\cite{Krizhevsky2012ImageNet}。
卷积神经网络主要是针对神经网络的缺点做了改进，总的概括起来为三个特性――局部连接、参数共享、池化采样。

\section{局部连接}

\begin{figure}[b]
\centering
\includegraphics[scale=0.7]{./Pictures/fc.jpg}
\caption{卷积神经网络的局部连接}
\label{fc}
\end{figure}

在传统的神经网络的图像分类问题中，如果我们要直接用原图像作为网络输入进行训练，那么每一个像素都要为之分配一个神经元。
也就是说一个$1000 \times 1000$像素的单通道灰度图像在输入层我们就需要$10^6$。
如果下一层有100个神经元输出，那么参数量又要扩大一百倍。
这样的网络如果最终要达到能够应用的程度，将会有巨大的参数量。

根据视觉神经相关研究的表明，我们的视觉神经元是有层次感。
低层的视觉神经元更加关注具体的局部细节（例如边缘，纹理等），而高层视觉神经元更加关注高层特征等（例如轮廓、空间关系等）DBJR
低层神经元的实现就是通过局部连接的思想实现，因为低层的视觉特征只需要关注很小的一个区域（patch）的图像，而不需要关注整幅图像。
这个被关注的区域就称为感受野，而实现方式就是通过局部连接。
例如我们只关心一个$10\times10$的区域，只需要100个参数就可以得到下一层神经元的输出。
单独拿出来看，这就是一个$10\times10$卷积核对图像中的这个patch做了一次卷积操作。
从图\ref{fc}我们可以看到，输出的这个神经元的值只和这个patch有关，并没有用到整幅图像的值，这就是局部连接。

\section{参数共享}
\begin{figure}[htb]
\centering
\includegraphics[scale=0.7]{./Pictures/share.jpg}
\caption{卷积神经网络的权值共享}
\label{share}
\end{figure}


局部连接可以一定程度上减少参数量，但是参数量依然很大。
上一小节提到卷积神经网络只关注感受野里的区域，然而一幅图像可以分割成许多这样的区域，每个区域都需要一个卷积核去卷积一遍。
如果每个区域的卷积核都是不一样的，那么最后依然会有很多训练的参数，并且和传统的全连接也并没有太大的改进。

为了进一步减少网络模型的参数量，卷积神经网络的第二特性为参数共享。
权值共享是指用一个卷积核去卷积一副图像里的所有patch。
如图\ref{share}所示，图像左边的部分有四个颜色方框，按照局部连接的思想，假如我们队四个patch 进行卷积运算（图中四个颜色的框），理论上我们需要$4\times 10 \times 10$个参数量。
如果对一幅图进行一次完整的循环卷积，那么参数量也是巨大的。
所以我们可以把这个卷积核的参数固定，用一个固定的卷积核去对整幅图像进行卷积，这样参数量就是一个卷积核的参数量。
权值共享不仅仅是直接的参数量大大缩小，它也是拥有充分论证的物理意义的，在图像处理领域一次卷积操作就是一次特征提取,例如边缘提取的Sobel算子等.

局部连接和参数共享合起来就是数学领域里的卷积计算，这也就是卷积神经网络的名字由来。
一个卷积核只能提取一种特征得到一副特征图（feature map），所以卷积神经网络会设置若干个卷积核提取更多不同的特征图）。
之后网络会将这些特征图融合起来，融合的权重以及卷积核的参数都是网络自动学习出来的。
 

\section{池化采样}
局部连接和权值共享都把参数量大大减少，但是却没有改变感受野。
