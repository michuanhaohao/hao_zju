\chapter{卷积神经网络}
卷积神经网络是深度学习的标志性成果，其前身是神经网络。
2012年，Hinton团队首次利用卷积神经网络Alexnet获得ImageNet挑战赛的冠军，并大幅提高识别准确度\cite{Krizhevsky2012ImageNet}。
卷积神经网络主要是针对神经网络的缺点做了改进，总的概括起来为三个特性――局部连接、参数共享、池化采样。

\section{局部连接}

\begin{figure}[b]
\centering
\includegraphics[scale=0.7]{./Pictures/fc.jpg}
\caption{卷积神经网络的局部连接}
\label{fc}
\end{figure}

在传统的神经网络的图像分类问题中，如果我们要直接用原图像作为网络输入进行训练，那么每一个像素都要为之分配一个神经元。
也就是说一个$1000 \times 1000$像素的单通道灰度图像在输入层我们就需要$10^6$。
如果下一层有100个神经元输出，那么参数量又要扩大一百倍。
这样的网络如果最终要达到能够应用的程度，将会有巨大的参数量。

根据视觉神经相关研究的表明，我们的视觉神经元是有层次感。
低层的视觉神经元更加关注具体的局部细节（例如边缘，纹理等），而高层视觉神经元更加关注高层特征等（例如轮廓、空间关系等）DBJR
低层神经元的实现就是通过局部连接的思想实现，因为低层的视觉特征只需要关注很小的一个区域（patch）的图像，而不需要关注整幅图像。
这个被关注的区域就称为感受野，而实现方式就是通过局部连接。
例如我们只关心一个$10\times10$的区域，只需要100个参数就可以得到下一层神经元的输出。
单独拿出来看，这就是一个$10\times10$卷积核对图像中的这个patch做了一次卷积操作。
从图\ref{fc}我们可以看到，输出的这个神经元的值只和这个patch有关，并没有用到整幅图像的值，这就是局部连接。

\section{参数共享}
\begin{figure}[htb]
\centering
\includegraphics[scale=0.7]{./Pictures/share.jpg}
\caption{卷积神经网络的权值共享}
\label{share}
\end{figure}


局部连接可以一定程度上减少参数量，但是参数量依然很大。
上一小节提到卷积神经网络只关注感受野里的区域，然而一幅图像可以分割成许多这样的区域，每个区域都需要一个卷积核去卷积一遍。
如果每个区域的卷积核都是不一样的，那么最后依然会有很多训练的参数，并且和传统的全连接也并没有太大的改进。

为了进一步减少网络模型的参数量，卷积神经网络的第二特性为参数共享。
权值共享是指用一个卷积核去卷积一副图像里的所有patch。
如图\ref{share}所示，图像左边的部分有四个颜色方框，按照局部连接的思想，假如我们队四个patch 进行卷积运算（图中四个颜色的框），理论上我们需要$4\times 10 \times 10$个参数量。
如果对一幅图进行一次完整的循环卷积，那么参数量也是巨大的。
所以我们可以把这个卷积核的参数固定，用一个固定的卷积核去对整幅图像进行卷积，这样参数量就是一个卷积核的参数量。
权值共享不仅仅是直接的参数量大大缩小，它也是拥有充分论证的物理意义的，在图像处理领域一次卷积操作就是一次特征提取,例如边缘提取的Sobel算子等.

局部连接和参数共享合起来就是数学领域里的卷积计算，这也就是卷积神经网络的名字由来。
一个卷积核只能提取一种特征得到一副特征图（feature map），所以卷积神经网络会设置若干个卷积核提取更多不同的特征图）。
之后网络会将这些特征图融合起来，融合的权重以及卷积核的参数都是网络自动学习出来的。
 

\section{池化采样}
\begin{figure}[htb]
\centering
\includegraphics[width=0.65\paperwidth]{./Pictures/pooling.jpg}
\caption{池化采样的常见的三种类型}
\label{pooling}
\end{figure}

局部连接和权值共享都把参数量大大减少，但是却没有改变感受野。
并且如果用卷积之后的feature map去训练一个分类器，例如 softmax 分类器，就需要特别大的计算量。
例如：对于一个$96\times96$像素的图像，假设我们已经学习得到了$400$个定义在$8\times8$输入上的特征，每一个特征和图像卷积都会得到一个$(96-8 + 1) \times (96-8 + 1) = 7921$维的卷积特征，由于有400个特征，所以每个样例(example)都会得到一个$892 \times 400 = 3,168,400$维的卷积特征向量。学习一个拥有超过 3 百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。

而池化采样是一个改变感受野并减少计算量的操作，使得网络可以实现多尺度多层次的特征提取。
池化对不同位置的特征进行聚合统计。
例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)。
这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。
池化通常分为平均池化(Mean pooling)、最大池化(Max pooling)和随机池化(Random pooling) (取决于计算池化的方法)。
顾名思义，平均池化就是用一个区域的平均值代替该区域的特征，而最大池化采用的是该区域的最大值，如图\ref{pooling}所示。
随机池化相对麻烦一点，做法是先把区域中的值归一化为概率矩阵，然后根据这个概率分布随机挑选一个值作为下一层的feature，随机池化并不常使用。

接下来我们来理解下pooling操作，首先是为什么是max和mean。
mean挺好理解的，我们做事情通常喜欢用平均值来代替一个集合的性能。
那么为什么是max而不是min了，这就要从神经网络的工作原理来理解，神经网络某个神经元的值特别大的话，说明这个神经元代表的属性被激活。
浅层的可以是直线纹理等特征，高层的可以代表猫狗之类的特征，我们需要把这些被激活的特征属性给传下，所以用的是max而不是min。
另外一个理解就是为什么需要pooling，前面提到pooling可以改变感受野大小。
这里假设每次都是$2\times2$大小的pooling，每做一次pooling之后feature map大小就变为了之前的一半，那么第二层做pooling的时候就用到了原始图片$4\times4$大小patch的信息.
越高层关注的区域就越大，也就是说感受野在逐渐变大.
浅层关注的是局部的细节特征，高层关注的是更加广阔的全局（实际上是更大的局部）特征，pooling把CNN的提取特征的层次感给体现了。